{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2d6ce-2b79-4fe3-b769-c10ae118fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import netCDF4\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import proplot as pplt\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "\n",
    "#For reproducibility set seeds\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "#loads n years of netcdf4-files into an array of xarrays\n",
    "def load_years(start_year, years):\n",
    "    #data is stored in a different directory, so get the parent first, then go to the data directory\n",
    "    parent_dir = Path.cwd().parent\n",
    "    data_dir = parent_dir / \"ERA5-downloader\"\n",
    "\n",
    "    monthly_data = list()\n",
    "\n",
    "    for year in range(start_year,(start_year + years)):  \n",
    "        print(\"Loading \" + str(year))\n",
    "        for month in range(1,13):\n",
    "            #for naming, make the month always two digits\n",
    "            if (month < 10):\n",
    "                month_filled = \"0\" + str(month)\n",
    "            else:\n",
    "                month_filled = str(month)\n",
    "                \n",
    "            file_name = str(year) + \"-\" + month_filled + \".nc\"\n",
    "            #print(file_name)\n",
    "            file_path = data_dir / file_name\n",
    "            monthly_data.append(xr.load_dataset(file_path, engine=\"netcdf4\"))\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "#Selects a random cell of a given size from an xarray\n",
    "def select_cell(data, cell_lat, cell_lon):\n",
    "    #check if xarray is larger than cell size\n",
    "    lon = tuple(dict(data[['longitude']].sizes).values())[0]\n",
    "    lat = tuple(dict(data[['latitude']].sizes).values())[0]\n",
    "    if (cell_lat > lat):\n",
    "        return data\n",
    "    if (cell_lon > lon):\n",
    "        return data\n",
    "\n",
    "    #now find indices of upper left corner, randomly between 0 and lat/lon - 5 or 6\n",
    "    lon_select = random.randint(0, lon - cell_lon)\n",
    "    lat_select = random.randint(0, lat - cell_lat)\n",
    "\n",
    "    return data.isel(longitude = slice(lon_select, lon_select + cell_lon), latitude = slice(lat_select, lat_select + cell_lat))\n",
    "\n",
    "#creates a random patch from the given data with the given width and height in pixels. Include a given number of steps before\n",
    "#and after the target sample. Returns a tuple of before and after, the latter including the central step\n",
    "def create_patch(data, patch_lat, patch_lon, pre_steps, post_steps, filter_prec):\n",
    "    #Is our selected patch suitable?\n",
    "    good_selection = False\n",
    "    while(good_selection == False):\n",
    "        sel = select_cell(data, patch_lat, patch_lon)\n",
    "        \n",
    "        #Now we want to select a single time step, with a number of steps before and after\n",
    "        #How many hours does our data include?\n",
    "        time = tuple(dict(sel[['time']].sizes).values())[0]\n",
    "        time_sel =  random.randint(pre_steps, time - post_steps - 1)\n",
    "        sel = sel.isel(time = slice(time_sel - pre_steps, time_sel + post_steps))\n",
    "\n",
    "        #Does our patch contain sufficient precipitation to learn from?\n",
    "        #Also convert to mm\n",
    "        if (filter_prec == True):\n",
    "            single = sel.isel(time = slice(pre_steps, pre_steps + post_steps)) #All steps of the prediction\n",
    "            patch_prec = single['tp'].sum(dim = [\"latitude\", \"longitude\", \"time\"])\n",
    "            #if it exceeds the limit, accept it and exit the loop\n",
    "            limit = post_steps * patch_lat * 0.0005\n",
    "            #currently 0.5 mm in one hour * 10 * 6\n",
    "            #print(patch_prec)\n",
    "            #print(limit)\n",
    "            if (patch_prec > limit):\n",
    "                good_selection = True\n",
    "        else:\n",
    "            good_selection = True\n",
    "                \n",
    "    return sel\n",
    "\n",
    "#write list to binary file\n",
    "def write_list(a_list, filename):\n",
    "    #store list in binary file so 'wb' mode\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "#read list to memory\n",
    "def read_list(filename):\n",
    "    # for reading also binary mode is important\n",
    "    with open(filename, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "#takes a list of xarrays and picks a number of samples from them without duplicates\n",
    "def get_patch_list(data, sample_count, pixel_width, pixel_height, prior_timesteps, past_timesteps, training_data):\n",
    "    patch_list = list()\n",
    "\n",
    "    no_duplicates = False\n",
    "    while(no_duplicates == False):\n",
    "        patch_list.clear()\n",
    "        for i in range(sample_count):\n",
    "            #Select a random month out of our xarray-list\n",
    "            patch_list.append(create_patch(random.choice(data), pixel_width, pixel_height, prior_timesteps, past_timesteps, training_data))\n",
    "   \n",
    "        #The risk of duplicates - same exact time and place - is roughly 2%. We do not want that, as samples would be overrepresented in the data\n",
    "        no_duplicates = True\n",
    "        for i in range(sample_count-1):\n",
    "            for j in range(i+1, sample_count):\n",
    "                #only compare the first xarray of the tuples. Since each tuple is consecutive, if the first is identical, the second is too\n",
    "                if (patch_list[i].equals(patch_list[j])):\n",
    "                    no_duplicates = False\n",
    "                    print(\"Duplicate at i = \" + str(i) + \" and j = \" + str(j))\n",
    "                    print(patch_list[i])\n",
    "                    print(patch_list[j])\n",
    "    return patch_list\n",
    "\n",
    "#Takes a list of datasets with the values u10, v10, t2m, sp, tp and throws away coordinates and the specific time while maintaining the sequence\n",
    "#Returns an array of shape [sample][feature][time][x][y]\n",
    "def xarray_dataset_list_to_numpy(dataset_list):\n",
    "\n",
    "    #Take the different values from each data set, stack them and put them into a new list\n",
    "    stacked_list = []\n",
    "    \n",
    "    for ds in dataset_list:\n",
    "        u10_array = ds['u10'].values\n",
    "        v10_array = ds['v10'].values\n",
    "        t2m_array = ds['t2m'].values\n",
    "        sp_array = ds['sp'].values\n",
    "        tp_array = ds['tp'].values * 1000 #convert to mm\n",
    "\n",
    "        stacked_list.append(np.stack((u10_array, v10_array, t2m_array, sp_array, tp_array), axis = 0))\n",
    "\n",
    "    return np.stack(stacked_list)\n",
    "    \n",
    "#find the minimum and maximum values for u10, v10, t2m, sp, tp and the difference between them\n",
    "#needed for normalization\n",
    "def find_min_max_ptp(data_array):\n",
    "    arr_max = np.max(data_array, axis = (0, 2, 3, 4))\n",
    "    arr_min = np.min(data_array, axis = (0, 2, 3, 4))\n",
    "    #shape of array: [u10,v10,t2m,sp,tp][min,max,range]\n",
    "    ptp = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        ptp[i] = arr_max[i] - arr_min[i]\n",
    "        if (ptp[i] == 0):\n",
    "            print(\"Error, no difference between min and max value!\")\n",
    "            print(\"Calculation will fail!\")\n",
    "\n",
    "    return np.stack((arr_min, arr_max, ptp), axis = 1)\n",
    "\n",
    "#Normalize all values apart from precipitation between 0 and 1\n",
    "def min_max_scaling(data_array, min_max_values):\n",
    "    data_shape = data_array.shape\n",
    "    print(data_shape)\n",
    "\n",
    "    for feature in range(data_shape[1]-1): #Do not scale tp!\n",
    "        data_array[:,feature,:,:,:] -= min_max_values[feature][0]\n",
    "        data_array[:,feature,:,:,:] /= min_max_values[feature][2]\n",
    "        \n",
    "    return data_array\n",
    "\n",
    "def print_stats(ds):\n",
    "    print(\"Min, max, mean, median\")\n",
    "    for feature in range(5):\n",
    "        print(str(np.min(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.max(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.mean(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.median(ds[:,feature,:,:,:])))\n",
    "\n",
    "def PGW(data, warming):\n",
    "    \"\"\"\n",
    "    Takes an existing, already generated dataset and applies warming to it\n",
    "\n",
    "    Args:\n",
    "        data (Array): Generated train/test numpy dataset as historical input \n",
    "                        of shape [*, 5, 54, x, x]\n",
    "        warming (float): The warming to be applied in K\n",
    "\n",
    "    Returns:\n",
    "        ds (Array): numpy dataset as of shape [*, 5, 54, x, x]\n",
    "    \"\"\"\n",
    "\n",
    "    ds = copy.deepcopy(data) #does this do what we want?\n",
    "    \n",
    "    print(ds.shape)\n",
    "    print(ds[0,4,0,0,0])\n",
    "\n",
    "    #u10,v10,t2m,sp,tp\n",
    "\n",
    "    #temperature\n",
    "    ds[:,2,:,:,:] += warming\n",
    "\n",
    "    #wind\n",
    "    #Pielke 2022:4 RCP 8.5 equals 5K warming\n",
    "    #Jung2019: RCP 8.5, all far future\n",
    "        #Mean: No change\n",
    "        #Skew and kurtosis too difficult\n",
    "        #Stdev increases by 10%\n",
    "        #-> preserve mean, preserve sign\n",
    "    stdev_factor = 1 + (0.02 * warming)\n",
    "\n",
    "    ds[:,0,:,:,:] = adjust_std_separately(ds[:,0,:,:,:], stdev_factor)\n",
    "    ds[:,1,:,:,:] = adjust_std_separately(ds[:,1,:,:,:], stdev_factor)\n",
    "    \n",
    "    #surface pressure\n",
    "    #Schmidt 2017:10575 - increase around 0.1 hPa/decade in AOI between 1980 and 2010\n",
    "    #NOAA 2023: 0.29 K/decade\n",
    "    # => 0.36 hPa/K\n",
    "    ds[:,3,:,:,:] += warming * 35.7 #1 hPa = 100 Pa\n",
    "    \n",
    "    #precipitation\n",
    "\n",
    "    prec_med = np.median(ds[:,4,:,:,:])\n",
    "    print(prec_med)\n",
    "    prec_mean = np.mean(ds[:,4,:,:,:])\n",
    "    print(prec_mean)\n",
    "    print(\"-------\")\n",
    "    \n",
    "    for sample in range(ds.shape[0]):\n",
    "        for hour in range(ds.shape[2]):\n",
    "            for x in range(ds.shape[3]):\n",
    "                for y in range(ds.shape[4]):\n",
    "                    prec = ds[sample, 4, hour, x, y]\n",
    "                    ds[sample, 4, hour, x, y] = prec * warming_prec(\n",
    "                        prec, warming, 2.1)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def adjust_std_separately(arr, factor):\n",
    "    pos_mask = arr > 0\n",
    "    neg_mask = arr < 0\n",
    "    \n",
    "    # Compute means separately\n",
    "    pos_mean = np.mean(arr[pos_mask]) if np.any(pos_mask) else 0\n",
    "    neg_mean = np.mean(arr[neg_mask]) if np.any(neg_mask) else 0\n",
    "    \n",
    "    # Center data around their respective means\n",
    "    pos_centered = arr[pos_mask] - pos_mean\n",
    "    neg_centered = arr[neg_mask] - neg_mean\n",
    "    \n",
    "    # Scale deviations separately\n",
    "    pos_centered *= factor\n",
    "    neg_centered *= factor\n",
    "    \n",
    "    # Re-add the respective means to maintain them\n",
    "    adjusted_arr = arr.copy()\n",
    "    adjusted_arr[pos_mask] = pos_centered + pos_mean\n",
    "    adjusted_arr[neg_mask] = neg_centered + neg_mean\n",
    "    \n",
    "    return adjusted_arr\n",
    "\n",
    "def warming_prec(prec, warming, prec_m):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    if (prec > 1.5*prec_m):\n",
    "        #print(\"Case 1\")\n",
    "        return pow(1.07, warming)\n",
    "    elif (prec < 0.5*prec_m):\n",
    "        #print(\"Case 2\")\n",
    "        return pow(0.93, warming)\n",
    "    else:\n",
    "        #print(\"Case 3\")\n",
    "        return (((pow(1.07, warming) - pow(0.93, warming)) / 2) * math.sin(\n",
    "            (math.pi / prec_m) * (prec - prec_m)) + ((pow(1.07, warming) + pow(0.93, warming)) / 2))\n",
    "\n",
    "numbers = np.linspace(0, 5, num = 200)\n",
    "scaling = np.empty(200)\n",
    "scaling2 = np.empty(200)\n",
    "scaling3 = np.empty(200)\n",
    "\n",
    "i = 0\n",
    "while (i < len(numbers)):\n",
    "    scaling[i] = warming_prec(numbers[i], 1, 2.1)\n",
    "    scaling2[i] = warming_prec(numbers[i], 2, 2.1)\n",
    "    scaling3[i] = warming_prec(numbers[i], 4, 2.1)\n",
    "    i = i + 1\n",
    "\n",
    "with pplt.rc.context(fontsize='11px'):\n",
    "    fig, ax = pplt.subplot(xlabel='Precipitation: mm/h', ylabel='Scale Factor',\n",
    "                           figheight='5cm', figwidth=\"14cm\")\n",
    "ax.plot(numbers, scaling, label = \"Warming 1째C\")\n",
    "ax.plot(numbers, scaling2, label = \"Warming 2째C\")\n",
    "ax.plot(numbers, scaling3, label = \"Warming 4째C\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "fig.savefig(\"transfer\")\n",
    "pplt.show()\n",
    "pplt.close()\n",
    "\n",
    "def plot_hist(ds, name):\n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, axes = pplt.subplots(nrows = 2, ncols = 3,figheight='8cm', figwidth=\"14cm\", share=False)\n",
    "    axes.format(yticks='null', abc = True)\n",
    "\n",
    "    wind_bins = np.linspace(-5, 5, 200)\n",
    "    t_bins = np.linspace(285, 310, 200)\n",
    "    p_bins = np.linspace(85000, 105000, 200)\n",
    "    prec_bins1 = np.linspace(0, 0.5, 200)\n",
    "    prec_bins2 = np.linspace(0.5, 10, 200)\n",
    "    \n",
    "    axes[0].hist(ds[:, 0, :, :, :].flatten(), bins = wind_bins) \n",
    "    axes[0].format(title = \"Wind u (10m)\", xlabel = \"m/s\")\n",
    "\n",
    "    axes[1].hist(ds[:, 1, :, :, :].flatten(), bins = wind_bins) \n",
    "    axes[1].format(title = \"Wind v (10m)\", xlabel = \"m/s\")\n",
    "\n",
    "    axes[2].hist(ds[:, 2, :, :, :].flatten(), bins = t_bins) \n",
    "    axes[2].format(title = \"Temperature (2m)\", xlabel = \"K\")\n",
    "\n",
    "    axes[3].hist(ds[:, 3, :, :, :].flatten(), bins = p_bins) \n",
    "    axes[3].format(title = \"Surface Pressure\", xlabel = \"Pa\")\n",
    "\n",
    "    axes[4].hist(ds[:, 4, :, :, :].flatten(), bins = prec_bins1) \n",
    "    axes[4].format(title = \"Precipitation < 0.5 mm/h\", xlabel = \"mm/h\")\n",
    "    \n",
    "    axes[5].hist(ds[:, 4, :, :, :].flatten(), bins = prec_bins2) \n",
    "    axes[5].format(title = \"Precipitation > 5 mm/h\", xlabel = \"mm/h\")\n",
    "    pplt.show()\n",
    "    fig.savefig(name)\n",
    "    pplt.close()\n",
    "\n",
    "def plot_hist_comp(ds1, ds2, name):\n",
    "\n",
    "    ds1 = np.swapaxes(ds1, 0, 1)\n",
    "    ds1 = np.reshape(ds1, (ds1.shape[0], ds1.shape[1]*ds1.shape[2]*ds1.shape[3]*ds1.shape[4]))\n",
    "    ds2 = np.swapaxes(ds2, 0, 1)\n",
    "    ds2 = np.reshape(ds2, (ds2.shape[0], ds2.shape[1]*ds2.shape[2]*ds2.shape[3]*ds2.shape[4]))\n",
    "    ds = np.stack((ds1, ds2), axis=2)\n",
    "   \n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, axes = pplt.subplots(nrows = 2, ncols = 3,figheight='8cm', figwidth=\"14cm\", share=False)\n",
    "\n",
    "    axes.format(yticks='null', abc = True)\n",
    "\n",
    "    wind_bins = np.linspace(-5, 5, 200)\n",
    "    t_bins = np.linspace(285, 310, 200)\n",
    "    p_bins = np.linspace(85000, 105000, 200)\n",
    "    prec_bins1 = np.linspace(0, 0.5, 200)\n",
    "    prec_bins2 = np.linspace(0.5, 10, 200)\n",
    "    \n",
    "    axes[0].hist(ds[0], bins = wind_bins, alpha = 0.5, filled=True, cycle=('indigo9', 'red9'),labels=(\"Base\", \"5K warming\")) \n",
    "    axes[0].format(title = \"Wind u (10m)\", xlabel = \"m/s\")\n",
    "\n",
    "    axes[1].hist(ds[1], bins = wind_bins, alpha = 0.5, filled=True, cycle=('indigo9', 'red9')) \n",
    "    axes[1].format(title = \"Wind v (10m)\", xlabel = \"m/s\")\n",
    "\n",
    "    axes[2].hist(ds[2], bins = t_bins, alpha = 0.5, filled=True, cycle=('indigo9', 'red9')) \n",
    "    axes[2].format(title = \"Temperature (2m)\", xlabel = \"K\")\n",
    "\n",
    "    axes[3].hist(ds[3], bins = p_bins, alpha = 0.5, filled=True, cycle=('indigo9', 'red9')) \n",
    "    axes[3].format(title = \"Surface Pressure\", xlabel = \"Pa\")\n",
    "\n",
    "    axes[4].hist(ds[4], bins = prec_bins1, alpha = 0.5, filled=True, cycle=('indigo9', 'red9')) \n",
    "    axes[4].format(title = \"Precipitation < 0.5 mm/h\", xlabel = \"mm/h\")\n",
    "    \n",
    "    axes[5].hist(ds[4], bins = prec_bins2, alpha = 0.5, filled=True, cycle=('indigo9', 'red9')) \n",
    "    axes[5].format(title = \"Precipitation > 0.5 mm/h\", xlabel = \"mm/h\")\n",
    "\n",
    "    fig.legend(loc='t')\n",
    "    \n",
    "    pplt.show()\n",
    "    fig.savefig(name)\n",
    "    pplt.close()\n",
    "\n",
    "\n",
    "#arr1 = read_list(\"test_array_10_1000_np\")\n",
    "#arr2 = read_list(\"PGW_10_1000_50_np\")\n",
    "#arr3 = read_list(\"train_array_10_10000_np\")\n",
    "\n",
    "#plot_hist_comp(arr1, arr2, \"Hist-test-data\")\n",
    "#plot_hist(arr3, \"Hist-training\")\n",
    "\n",
    "#print_stats(arr1)\n",
    "#print_stats(arr2)\n",
    "#print_stats(arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dc28f-a015-4ffc-b3d6-d220aa1b4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell actually calls all functions needed to transform the data\n",
    "#This gives us several years of monthly xarrays in one list\n",
    "train_data_A = load_years(1980, 2)\n",
    "train_data_B = load_years(2020, 2)\n",
    "test_data = load_years(2000, 2)\n",
    "print(\"Loading finished\")\n",
    "\n",
    "#How many patches do we want each?\n",
    "train_number = 5000 #for each of the two datasets\n",
    "test_number = 1000\n",
    "#How large should our patches be? One pixel = 0.25째\n",
    "pixel_width = 10\n",
    "pixel_height = 10\n",
    "#How many hours before and after do we want?\n",
    "prior_timesteps = 48\n",
    "past_timesteps = 6\n",
    "\n",
    "print(\"Creating train list\")\n",
    "train_list = get_patch_list(train_data_A, train_number, pixel_width, pixel_height, prior_timesteps, past_timesteps, True) \\\n",
    "+ get_patch_list(train_data_B, train_number, pixel_width, pixel_height, prior_timesteps, past_timesteps, True)\n",
    "print(\"Train list created\")\n",
    "print(\"Creating test list\")\n",
    "test_list = get_patch_list(test_data, test_number, pixel_width, pixel_height, prior_timesteps, past_timesteps, False)\n",
    "print(\"Test list created\")\n",
    "\n",
    "train_array = xarray_dataset_list_to_numpy(train_list)\n",
    "test_array = xarray_dataset_list_to_numpy(test_list)\n",
    "\n",
    "#Due to conversion errors some precipiation values are slightly below 0, so we clip them to 0\n",
    "train_array[:,4,:,:,:] = np.clip(train_array[:,4,:,:,:], 0, None)\n",
    "test_array[:,4,:,:,:] = np.clip(test_array[:,4,:,:,:], 0, None)\n",
    "#Also save the non-scaled arrays\n",
    "write_list(train_array, \"train_array_10_10000_np\")\n",
    "write_list(test_array, \"test_array_10_2000_np\")\n",
    "\n",
    "PGW_array_05 = PGW(test_array, 0.5)\n",
    "PGW_array_10 = PGW(test_array, 1)\n",
    "PGW_array_20 = PGW(test_array, 2)\n",
    "PGW_array_30 = PGW(test_array, 3)\n",
    "PGW_array_40 = PGW(test_array, 4)\n",
    "PGW_array_50 = PGW(test_array, 5)\n",
    "\n",
    "write_list(PGW_array_05, \"PGW_10_1000_05_np\")\n",
    "write_list(PGW_array_10, \"PGW_10_1000_10_np\")\n",
    "write_list(PGW_array_20, \"PGW_10_1000_20_np\")\n",
    "write_list(PGW_array_30, \"PGW_10_1000_30_np\")\n",
    "write_list(PGW_array_40, \"PGW_10_1000_40_np\")\n",
    "write_list(PGW_array_50, \"PGW_10_1000_50_np\")\n",
    "\n",
    "#We want to use the min-max-values of the training data to scale both train and test data -> no data flow from test to train\n",
    "min_max = find_min_max_ptp(train_array)\n",
    "write_list(min_max)\n",
    "\n",
    "print(\"Now scaling\")\n",
    "train_array_scaled = min_max_scaling(copy.deepcopy(train_array), min_max)\n",
    "test_array_scaled = min_max_scaling(copy.deepcopy(test_array), min_max)\n",
    "\n",
    "PGW_05_scaled = min_max_scaling(copy.deepcopy(PGW_array_05), min_max)\n",
    "PGW_10_scaled = min_max_scaling(copy.deepcopy(PGW_array_10), min_max)\n",
    "PGW_20_scaled = min_max_scaling(copy.deepcopy(PGW_array_20), min_max)\n",
    "PGW_30_scaled = min_max_scaling(copy.deepcopy(PGW_array_30), min_max)\n",
    "PGW_40_scaled = min_max_scaling(copy.deepcopy(PGW_array_40), min_max)\n",
    "PGW_50_scaled = min_max_scaling(copy.deepcopy(PGW_array_50), min_max)\n",
    "\n",
    "write_list(train_array_scaled, \"train_array_10_10000\")\n",
    "write_list(test_array_scaled, \"test_array_10_2000\")\n",
    "\n",
    "write_list(PGW_05_scaled, \"PGW_10_1000_05\")\n",
    "write_list(PGW_10_scaled, \"PGW_10_1000_10\")\n",
    "write_list(PGW_20_scaled, \"PGW_10_1000_20\")\n",
    "write_list(PGW_30_scaled, \"PGW_10_1000_30\")\n",
    "write_list(PGW_40_scaled, \"PGW_10_1000_40\")\n",
    "write_list(PGW_50_scaled, \"PGW_10_1000_50\")\n",
    "print(\"Lists saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
