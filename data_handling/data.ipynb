{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2d6ce-2b79-4fe3-b769-c10ae118fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import netCDF4\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import proplot as pplt\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "\n",
    "#For reproducibility set seeds\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "#loads n years of netcdf4-files into an array of xarrays\n",
    "def load_years(start_year, years):\n",
    "    #data is stored in a different directory, so get the parent first, then go to the data directory\n",
    "    parent_dir = Path.cwd().parent\n",
    "    data_dir = parent_dir / \"ERA5-downloader\"\n",
    "\n",
    "    monthly_data = list()\n",
    "\n",
    "    for year in range(start_year,(start_year + years)):  \n",
    "        print(\"Loading \" + str(year))\n",
    "        for month in range(1,13):\n",
    "            #for naming, make the month always two digits\n",
    "            if (month < 10):\n",
    "                month_filled = \"0\" + str(month)\n",
    "            else:\n",
    "                month_filled = str(month)\n",
    "                \n",
    "            file_name = str(year) + \"-\" + month_filled + \".nc\"\n",
    "            #print(file_name)\n",
    "            file_path = data_dir / file_name\n",
    "            monthly_data.append(xr.load_dataset(file_path, engine=\"netcdf4\"))\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "#Selects a random cell of a given size from an xarray\n",
    "def select_cell(data, cell_lat, cell_lon):\n",
    "    #check if xarray is larger than cell size\n",
    "    lon = tuple(dict(data[['longitude']].sizes).values())[0]\n",
    "    lat = tuple(dict(data[['latitude']].sizes).values())[0]\n",
    "    if (cell_lat > lat):\n",
    "        return data\n",
    "    if (cell_lon > lon):\n",
    "        return data\n",
    "\n",
    "    #now find indices of upper left corner, randomly between 0 and lat/lon - 5 or 6\n",
    "    lon_select = random.randint(0, lon - cell_lon)\n",
    "    lat_select = random.randint(0, lat - cell_lat)\n",
    "\n",
    "    return data.isel(longitude = slice(lon_select, lon_select + cell_lon), latitude = slice(lat_select, lat_select + cell_lat))\n",
    "\n",
    "#creates a random patch from the given data with the given width and height in pixels. Include a given number of steps before\n",
    "#and after the target sample. Returns a tuple of before and after, the latter including the central step\n",
    "def create_patch(data, patch_lat, patch_lon, pre_steps, post_steps, filter_prec):\n",
    "    #Is our selected patch suitable?\n",
    "    good_selection = False\n",
    "    while(good_selection == False):\n",
    "        sel = select_cell(data, patch_lat, patch_lon)\n",
    "        \n",
    "        #Now we want to select a single time step, with a number of steps before and after\n",
    "        #How many hours does our data include?\n",
    "        time = tuple(dict(sel[['time']].sizes).values())[0]\n",
    "        time_sel =  random.randint(pre_steps, time - post_steps - 1)\n",
    "        sel = sel.isel(time = slice(time_sel - pre_steps, time_sel + post_steps))\n",
    "\n",
    "        #Does our patch contain sufficient precipitation to learn from?\n",
    "        #Also convert to mm\n",
    "        if (filter_prec == True):\n",
    "            single = sel.isel(time = slice(pre_steps, pre_steps + post_steps)) #All steps of the prediction\n",
    "            patch_prec = single['tp'].sum(dim = [\"latitude\", \"longitude\", \"time\"])\n",
    "            #if it exceeds the limit, accept it and exit the loop\n",
    "            limit = post_steps * patch_lat * 0.0005\n",
    "            #currently 0.5 mm in one hour * 10 * 6\n",
    "            #print(patch_prec)\n",
    "            #print(limit)\n",
    "            if (patch_prec > limit):\n",
    "                good_selection = True\n",
    "        else:\n",
    "            good_selection = True\n",
    "                \n",
    "    return sel\n",
    "\n",
    "#write list to binary file\n",
    "def write_list(a_list, filename):\n",
    "    #store list in binary file so 'wb' mode\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "#read list to memory\n",
    "def read_list(filename):\n",
    "    # for reading also binary mode is important\n",
    "    with open(filename, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "#takes a list of xarrays and picks a number of samples from them without duplicates\n",
    "def get_patch_list(data, sample_count, pixel_width, pixel_height, prior_timesteps, past_timesteps, training_data):\n",
    "    patch_list = list()\n",
    "\n",
    "    no_duplicates = False\n",
    "    while(no_duplicates == False):\n",
    "        patch_list.clear()\n",
    "        for i in range(sample_count):\n",
    "            #Select a random month out of our xarray-list\n",
    "            patch_list.append(create_patch(random.choice(data), pixel_width, pixel_height, prior_timesteps, past_timesteps, training_data))\n",
    "   \n",
    "        #The risk of duplicates - same exact time and place - is roughly 2%. We do not want that, as samples would be overrepresented in the data\n",
    "        no_duplicates = True\n",
    "        for i in range(sample_count-1):\n",
    "            for j in range(i+1, sample_count):\n",
    "                #only compare the first xarray of the tuples. Since each tuple is consecutive, if the first is identical, the second is too\n",
    "                if (patch_list[i].equals(patch_list[j])):\n",
    "                    no_duplicates = False\n",
    "                    print(\"Duplicate at i = \" + str(i) + \" and j = \" + str(j))\n",
    "                    print(patch_list[i])\n",
    "                    print(patch_list[j])\n",
    "    return patch_list\n",
    "\n",
    "#Takes a list of datasets with the values u10, v10, t2m, sp, tp and throws away coordinates and the specific time while maintaining the sequence\n",
    "#Returns an array of shape [sample][feature][time][x][y]\n",
    "def xarray_dataset_list_to_numpy(dataset_list):\n",
    "\n",
    "    #Take the different values from each data set, stack them and put them into a new list\n",
    "    stacked_list = []\n",
    "    \n",
    "    for ds in dataset_list:\n",
    "        u10_array = ds['u10'].values\n",
    "        v10_array = ds['v10'].values\n",
    "        t2m_array = ds['t2m'].values\n",
    "        sp_array = ds['sp'].values\n",
    "        tp_array = ds['tp'].values * 1000 #convert to mm\n",
    "\n",
    "        stacked_list.append(np.stack((u10_array, v10_array, t2m_array, sp_array, tp_array), axis = 0))\n",
    "\n",
    "    return np.stack(stacked_list)\n",
    "    \n",
    "#find the minimum and maximum values for u10, v10, t2m, sp, tp and the difference between them\n",
    "#needed for normalization\n",
    "def find_min_max_ptp(data_array):\n",
    "    arr_max = np.max(data_array, axis = (0, 2, 3, 4))\n",
    "    arr_min = np.min(data_array, axis = (0, 2, 3, 4))\n",
    "    #shape of array: [u10,v10,t2m,sp,tp][min,max,range]\n",
    "    ptp = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        ptp[i] = arr_max[i] - arr_min[i]\n",
    "        if (ptp[i] == 0):\n",
    "            print(\"Error, no difference between min and max value!\")\n",
    "            print(\"Calculation will fail!\")\n",
    "\n",
    "    return np.stack((arr_min, arr_max, ptp), axis = 1)\n",
    "\n",
    "#Normalize all values apart from precipitation between 0 and 1\n",
    "def min_max_scaling(data_array, min_max_values):\n",
    "    data_shape = data_array.shape\n",
    "    print(data_shape)\n",
    "\n",
    "    for feature in range(data_shape[1]-1): #Do not scale tp!\n",
    "        data_array[:,feature,:,:,:] -= min_max_values[feature][0]\n",
    "        data_array[:,feature,:,:,:] /= min_max_values[feature][2]\n",
    "        \n",
    "    return data_array\n",
    "\n",
    "def print_stats(ds):\n",
    "    print(\"Min, max, mean, median\")\n",
    "    for feature in range(5):\n",
    "        print(str(np.min(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.max(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.mean(ds[:,feature,:,:,:])) + \" | \" +\n",
    "              str(np.median(ds[:,feature,:,:,:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
