{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853734b9-2e74-4e2a-a57a-a8a05ebc3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import xarray\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import scores.spatial\n",
    "import pysteps\n",
    "\n",
    "import proplot as pplt\n",
    "\n",
    "#Custom implementation of pytorch's dataset\n",
    "class ERA5Dataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.file_dir = \"data_handling\"\n",
    "        \n",
    "        #data is stored in a different directory, so get the parent first, then go to the data directory\n",
    "        parent_dir = Path.cwd().parent\n",
    "        data_dir = parent_dir / self.file_dir\n",
    "        file_path = data_dir / filename\n",
    "    \n",
    "        #for reading also binary mode is important\n",
    "        with open(file_path, 'rb') as fp:\n",
    "            np_list = pickle.load(fp)\n",
    "\n",
    "\n",
    "        #Samples are automatically handled and the number of channels is static\n",
    "        #Order is prior steps, prediction steps, x, y\n",
    "        #Since time is saved into one dimension, the exact split is lost\n",
    "        #This solution contains redundancy and should possibly be improved in the future\n",
    "        pred_steps = 6 #How many of our steps do we want to predict in the future?\n",
    "        self.data_shape = (np_list.shape[2] - pred_steps, pred_steps, np_list.shape[3], np_list.shape[4])\n",
    "        \n",
    "        #Split the data into x and y as a tuple, with the last timesteps being the label\n",
    "        ds = torch.from_numpy(np_list).to(torch.float32)\n",
    "        self.data, self.label = torch.split(ds, [self.data_shape[0],self.data_shape[1]], dim = 2)\n",
    "\n",
    "        #How many values does our prediction contain when flattened into one dimension?\n",
    "        self.pred_size = self.data_shape[1] * self.data_shape[2] * self.data_shape[3]\n",
    "\n",
    "        #As a label, we only want precipitation\n",
    "        self.label = self.label[:,4,:,:,:]\n",
    "        self.label = torch.unsqueeze(self.label, 1)\n",
    "        print(\"Data Shape: Samples, channels, time, x, y\")\n",
    "        print(\"Data: \" + str(self.data.shape))\n",
    "        print(\"Label: \" + str(self.label.shape))\n",
    "\n",
    "        #for running the model on CUDA, we need to move it\n",
    "        self.data = self.data.to(device)\n",
    "        self.label = self.label.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "    def get_data_shape(self):\n",
    "        return self.data_shape\n",
    "\n",
    "    def get_pred_size(self):\n",
    "        return self.pred_size\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "train_data = ERA5Dataset(\"train_array_10_10000\")\n",
    "test_data = ERA5Dataset(\"test_array_10_1000\")\n",
    "\n",
    "data_shape = train_data.get_data_shape()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, data_shape):\n",
    "        super().__init__()\n",
    "        features = 5\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.res_net1 = self._res_layer_set(features, features*2)\n",
    "        self.res_conv1 = nn.Conv3d(features, features*2, kernel_size=(1), padding =(0), stride = (1))\n",
    "\n",
    "        self.res_net2 = self._res_layer_set(features*2, features*4)\n",
    "        self.res_conv2 = nn.Conv3d(features*2, features*4, kernel_size=(1), padding =(0), stride = (1))\n",
    "\n",
    "        self.res_net3 = self._res_layer_set(features*4, features*8)\n",
    "        self.res_conv3 = nn.Conv3d(features*4, features*8, kernel_size=(1), padding =(0), stride = (1))\n",
    "\n",
    "        self.res_net4 = self._res_layer_set(features*8, features*16)\n",
    "        self.res_conv4 = nn.Conv3d(features*8, features*16, kernel_size=(1), padding =(0), stride = (1))  \n",
    "\n",
    "        self.conv_layer1 = self._conv_layer_set(features*16, features*8)\n",
    "\n",
    "        self.res_net5 = self._res_layer_set(features*8, features*4)\n",
    "        self.res_conv5 = nn.Conv3d(features*8, features*4, kernel_size=(1), padding =(0), stride = (1)) \n",
    "        \n",
    "        self.conv_layer2= self._conv_layer_set(features*4, features*2)\n",
    "\n",
    "        self.res_net6 = self._res_layer_set(features*2, features*2)\n",
    "        self.res_conv6 = nn.Conv3d(features*2, features*2, kernel_size=(1), padding =(0), stride = (1)) \n",
    "        \n",
    "        self.conv_layer3 = self._conv_layer_set(features*2, features)\n",
    "\n",
    "        self.res_net7 = self._res_layer_set(features, features)\n",
    "        self.res_conv7 = nn.Conv3d(features, features, kernel_size=(1), padding =(0), stride = (1)) \n",
    "\n",
    "        self.refl_pad = nn.ReflectionPad3d(1)\n",
    "        self.conv_layer4 = nn.Conv3d(features, 1, kernel_size=(3), padding=(0), stride = (1))\n",
    "\n",
    "        #initialize layers\n",
    "    def _res_layer_set(self, in_c, out_c):\n",
    "        conv_stack = nn.Sequential(\n",
    "            nn.ReflectionPad3d(1),\n",
    "            nn.Conv3d(in_c, in_c, kernel_size=(3), padding = (0), stride = (1)),\n",
    "            nn.BatchNorm3d(in_c),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad3d(1),\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(3), padding = (0), stride = (1)),\n",
    "            nn.BatchNorm3d(out_c))\n",
    "        return conv_stack\n",
    "\n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(\n",
    "            nn.ReflectionPad3d(1),\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(3), padding=(0), stride = (1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad3d(1),\n",
    "            nn.AvgPool3d(kernel_size = (3), padding = (0), stride = (2, 1, 1))\n",
    "        )\n",
    "        return conv_layer\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #First ResNet, features 5->10\n",
    "        res = self.res_conv1(x)\n",
    "        out = self.res_net1(x)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Second ResNeT, features 10->20\n",
    "        res = self.res_conv2(out)\n",
    "        out = self.res_net2(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Third ResNeT, features 20->40\n",
    "        res = self.res_conv3(out)\n",
    "        out = self.res_net3(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Fourth ResNeT, features 40\n",
    "        res = self.res_conv4(out)\n",
    "        out = self.res_net4(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Downsample the time dimension, 48 -> 24\n",
    "        #Features 40->20\n",
    "        out = self.conv_layer1(out)\n",
    "        #ResNet\n",
    "        res = self.res_conv5(out)\n",
    "        out = self.res_net5(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Downsample the time dimension, 24 -> 12\n",
    "        #Features 20->10\n",
    "        out = self.conv_layer2(out)\n",
    "        #ResNet\n",
    "        res = self.res_conv6(out)\n",
    "        out = self.res_net6(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Downsample the time dimension, 12 -> 6\n",
    "        #Features 10->5\n",
    "        out = self.conv_layer3(out)\n",
    "        #ResNet\n",
    "        res = self.res_conv7(out)\n",
    "        out = self.res_net7(out)\n",
    "        out = self.relu(out + res)\n",
    "\n",
    "        #Features 5 -> 1\n",
    "        out = self.refl_pad(out)\n",
    "        out = self.conv_layer4(out)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = ResNet(data_shape)\n",
    "model = model.to(device)\n",
    "\n",
    "#summary(model, input_size=(batch_size, 5, 48, 10, 10))\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.nn.functional.softplus(-2. * x) - math.log(2.0)\n",
    "    return torch.mean(_log_cosh(y_pred - y_true))\n",
    "\n",
    "#As suggested by Ayzel 2020\n",
    "class LogCoshLossOld(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return log_cosh_loss(y_pred, y_true)\n",
    "\n",
    "#Make input non-static, sample from normal deviation\n",
    "class SpatiotemporalPrecipitationLoss(nn.Module):\n",
    "    def __init__(self, weight_factor=10.0, threshold=0.1):\n",
    "        \"\"\"\n",
    "        A spatiotemporal loss function that combines weighted MSE for precipitation values.\n",
    "        Args:\n",
    "            weight_factor (float): The weight for higher precipitation regions.\n",
    "            threshold (float): The threshold above which precipitation is considered significant.\n",
    "        \"\"\"\n",
    "        super(SpatiotemporalPrecipitationLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')  # 'none' to compute per element\n",
    "        self.weight_factor = weight_factor\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute the weighted spatiotemporal MSE loss.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted precipitation values (batch_size, time_steps, channels, height, width).\n",
    "            target (torch.Tensor): Ground truth precipitation values (batch_size, time_steps, channels, height, width).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        # Compute the base MSE loss for each pixel in each timestep\n",
    "        mse = self.mse_loss(pred, target)  # Shape: (batch_size, time_steps, channels, height, width)\n",
    "\n",
    "        # Create a mask for areas with significant precipitation in the target\n",
    "        weight_mask = (target > self.threshold).float()  # Same shape as target\n",
    "\n",
    "        # Create weighted loss: more weight for higher precipitation areas\n",
    "        weighted_mse = self.weight_factor * weight_mask * mse\n",
    "\n",
    "        # Average the weighted loss over all dimensions (batch, time, spatial)\n",
    "        combined_loss = torch.mean(mse + weighted_mse)\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    model.train()\n",
    "\n",
    "    loss_list = list() #use this to calculate the average\n",
    "    \n",
    "    #This is for plotting the same samples\n",
    "    train_batch = read_file(\"train_10_batch\")\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    if((epochs%5 == 0)):\n",
    "        pred = model(train_batch[0])\n",
    "        \n",
    "        #these are the final two timesteps before the prediction\n",
    "        pre_plot = train_batch[0][:,4,-2:,:,:].clone().detach()\n",
    "        pred_plot = pred.clone().detach()\n",
    "        y_plot = train_batch[1].clone().detach() \n",
    "        \n",
    "        plot_prediction(pre_plot, pred_plot, y_plot, epochs, \"train_\", 2)\n",
    "        plot_prediction(pre_plot, pred_plot, y_plot, epochs, \"train_\", 5)\n",
    "    \n",
    "    return np.average(loss_list)\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, epochs):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #This is for plotting the same samples\n",
    "    test_batch = read_file(\"test_10_batch\")\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        #Always plot the same samples\n",
    "        if((epochs%5 == 0)):\n",
    "            pred = model(test_batch[0])\n",
    "            #these are the final two timesteps before the prediction\n",
    "            pre = test_batch[0][:,4,-2:,:,:]     \n",
    "            plot_prediction(pre, pred, test_batch[1], epochs, \"test_\", 12)\n",
    "            plot_prediction(pre, pred, test_batch[1], epochs, \"test_\", 23)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "def train_model(epochs, loss, optim, name):\n",
    "    \"\"\"\n",
    "    Function that contains a model training run\n",
    "    Args:\n",
    "        epochs (int): The number of epochs that should be trained\n",
    "        loss (nn.module): The loss function to be used\n",
    "        optim (nn.optim): The optimizer to be used\n",
    "        name (String): The name of this model run\n",
    "    \"\"\"\n",
    "    train_loss_list = list()\n",
    "    test_loss_list = list()\n",
    "    best_test_loss = 0\n",
    "    best_train_epoch = 0\n",
    "    best_test_epoch = 0\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss_list.append(train_loop(train_dataloader, model, loss_fn, optimizer, t))\n",
    "        test_loss_list.append(test_loop(test_dataloader, model, loss_fn, t))\n",
    "\n",
    "        file_path = Path.cwd() / name / (\"model_state_\" + str(t))\n",
    "        file_path.parent.mkdir(exist_ok=True, parents=True) #create the subdir if it does not exist\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        \n",
    "        write_file(train_loss_list, \"train_loss_list\", name)\n",
    "        write_file(test_loss_list, \"test_loss_list\", name)\n",
    "        write_file(t, \"epoch_count\", name)\n",
    "    \n",
    "        if(t%10 == 0):\n",
    "            plot_train_test_loss(train_loss_list, test_loss_list)\n",
    "    \n",
    "        #Early stopping:\n",
    "        stopping_epochs = 50 #How many epochs do we train past a local minimum?\n",
    "        epoch_counter = 0 #How many epochs are we past the local minimum?\n",
    "        if (t == 0):\n",
    "            best_test_loss = test_loss_list[0]\n",
    "        else:\n",
    "            if (test_loss_list[t] < best_test_loss):\n",
    "                #print(\"Model has improved!\")\n",
    "                epoch_counter = 0\n",
    "            else:\n",
    "                print(\"Model has not improved!\")\n",
    "                if (epoch_counter < stopping_epochs):\n",
    "                    epoch_counter = epoch_counter + 1\n",
    "                else:\n",
    "                    print(\"Early stop!\")\n",
    "                    break     \n",
    "    print(\"Done!\")\n",
    "\n",
    "def predict(modelname, epoch, data, index = -1):\n",
    "    \"\"\"\n",
    "    Function that loads a model state and predict the output.\n",
    "    If more than 48 timesteps are given, the last ones are discarded\n",
    "    If an index is given, only that index of the data is plotted\n",
    "    Args:\n",
    "        model (String): The name of the model to be used, e.g. D or E\n",
    "        epoch (int): The epoch of the state to be loaded\n",
    "        data (torch.Tensor): The data to be predicted (batch_size, time_steps, channels, height, width)\n",
    "        index (int): The specific index of the batch to be used\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Prediction (batch_size, time_steps, channels, height, width)\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = Path.cwd() / modelname / (\"model_state_\" + str(epoch))\n",
    "    print(file_path)\n",
    "    model.load_state_dict(torch.load(file_path, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    if (index >= 0):\n",
    "        data = data[index]\n",
    "        data = torch.unsqueeze(data,0) #Keep first dimension\n",
    "        \n",
    "    output = torch.zeros(data.shape[0], 1, 6, data.shape[3], data.shape[4])\n",
    "\n",
    "    data = data.cuda()\n",
    "    output = output.cuda()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i in range(data.shape[0]):\n",
    "            output[i] = model(torch.unsqueeze(data[i],0))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def save_batch(dataloader, filename):\n",
    "    \"\"\"\n",
    "    Takes the next batch of a dataloader and saves it via pickle. This is to allow testing with the same data without shuffling\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.dataloader)\n",
    "        filename (String): The name to be used for saving the file\n",
    "    \"\"\"\n",
    "    data = next(iter(dataloader))\n",
    "    write_file(data, filename)\n",
    "    \n",
    "#write function that finds lowest train/test loss and plots that  \n",
    "def find_lowest_loss(model):\n",
    "    '''\n",
    "    Prints the epochs with the lowest train and test loss\n",
    "\n",
    "    Args:\n",
    "        model (String): The name of the model to be analysed\n",
    "    '''\n",
    "\n",
    "    train_loss = read_file(\"train_loss_list\", model)\n",
    "    test_loss = read_file(\"test_loss_list\", model)\n",
    "\n",
    "    best_train_epoch = train_loss.index(min(train_loss))\n",
    "    best_test_epoch = test_loss.index(min(test_loss))\n",
    "\n",
    "    print(\"The best train loss of model \" + model + \" is at epoch \" + str(best_train_epoch))\n",
    "    print(\"The best test loss of model \" + model + \" is at epoch \" + str(best_test_epoch))\n",
    "\n",
    "def np_to_torch(filename):\n",
    "    file_dir = \"data_handling\"\n",
    "        \n",
    "    #data is stored in a different directory, so get the parent first, then go to the data directory\n",
    "    parent_dir = Path.cwd().parent\n",
    "    data_dir = parent_dir / file_dir\n",
    "    file_path = data_dir / filename\n",
    "    \n",
    "    #for reading also binary mode is important\n",
    "    with open(file_path, 'rb') as fp:\n",
    "        np_list = pickle.load(fp)\n",
    "\n",
    "    pred_steps = 6 #How many of our steps do we want to predict in the future?\n",
    "        \n",
    "    #Split the data into x and y as a tuple, with the last timesteps being the label\n",
    "    ds = torch.from_numpy(np_list).to(torch.float32)\n",
    "    data, label = torch.split(ds, [np_list.shape[2] - pred_steps,pred_steps], dim = 2)\n",
    "\n",
    "    #As a label, we only want precipitation\n",
    "    label = label[:,4,:,:,:]\n",
    "    label = torch.unsqueeze(label, 1)\n",
    "\n",
    "    return [data, label]\n",
    "\n",
    "def score_fss(target, prediction):\n",
    "    \"\"\"\n",
    "    Calculates the fraction skill score of the prediction vs \n",
    "    the target of window sizes 1, 2, 4 and threshholds 0.1 and 1\n",
    "    Args:\n",
    "        target (tensor): The prediction target, (batch, prec, time, x, y)\n",
    "        prediction (tensor): The prediction, (batch, prec, time, x, y)\n",
    "\n",
    "    Returns: \n",
    "        fss: The fraction skill score averaged over the entire batch,\n",
    "            np array of shape (window_sizes, thresholds)\n",
    "    \"\"\"    \n",
    "    #Squeeze precipiation\n",
    "    target = torch.squeeze(target)\n",
    "    prediction = torch.squeeze(prediction)\n",
    "    \n",
    "    target =  target.cpu().detach().numpy()\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "\n",
    "    if(target.shape != prediction.shape):\n",
    "        print(\"Error: Shape mismatch!\")\n",
    "\n",
    "    shape = target.shape\n",
    "\n",
    "    window_sizes = [(1,1), (4,4)]\n",
    "    thresholds = [0.1, 1, 2]\n",
    "\n",
    "    fss = np.empty((shape[0], len(window_sizes), len(thresholds))) #this is where we will save the scores\n",
    "\n",
    "    for batch in range(shape[0]):\n",
    "        #scores needs an xarray DataArray\n",
    "        current_target = xarray.DataArray(data = target[batch], dims = [\"time\", \"x\", \"y\"])\n",
    "        current_prediction = xarray.DataArray(data = prediction[batch], dims = [\"time\", \"x\", \"y\"])\n",
    "\n",
    "        w_count = 0\n",
    "        for window in window_sizes:\n",
    "            th_count = 0\n",
    "            for th in thresholds:\n",
    "                fss[batch][w_count][th_count] = scores.spatial.fss_2d(\n",
    "                    fcst = current_prediction, obs = current_target, \n",
    "                    event_threshold = th, window_size = window, spatial_dims = (\"x\", \"y\")).to_numpy()\n",
    "                th_count = th_count + 1\n",
    "            w_count = w_count + 1\n",
    "    fss = np.mean(fss, axis=0)\n",
    "    return fss\n",
    "    \n",
    "def score_rapsd(target, norm=False):\n",
    "    \"\"\"\n",
    "    Calculates the average difference of the radially averaged power spectral density of a tensors\n",
    "    Args:\n",
    "        target (tensor): The prediction target, (batch, prec, time, x, y)\n",
    "        norm (Bool): Whether to normalise\n",
    "    Returns:\n",
    "        Output (Array, 2x5): The RAPSD per frequency in dB, The list of wavelengths in km\n",
    "    \"\"\"\n",
    "    \n",
    "    target = target.cpu().detach().numpy()\n",
    "    shape = target.shape\n",
    "\n",
    "    output = np.empty((shape[0], shape[2], 5))\n",
    "    freq = None\n",
    "\n",
    "    for batch in range(shape[0]):\n",
    "        for hour in range(shape[2]):\n",
    "            image = target[batch][0][hour]\n",
    "            noise = np.random.normal(loc = 0, scale=0.0001, size=image.shape)\n",
    "            out1, freq = pysteps.utils.spectral.rapsd(field = image + noise, fft_method = np.fft, return_freq = True, normalize=norm)\n",
    "            output[batch][hour] = out1\n",
    "\n",
    "    output = np.nanmean(output, axis=(0, 1)) #the average of all batch elements and hours\n",
    "    #Convert to dB\n",
    "    output = np.log10(output)\n",
    "\n",
    "    #Convert frequency to wavelength\n",
    "    #The grid size of ERA5 Single Level is 27.8km at the equator\n",
    "    wavelength = 27.8 / freq\n",
    "    #See Ruzanski, 2010:2301\n",
    "    \n",
    "    return np.transpose(np.column_stack((wavelength, output)))\n",
    "\n",
    "def plot_rapsd(input, filename, height=\"7cm\"):\n",
    "    '''\n",
    "    Plots the RAPSD of the target and one or many predictions\n",
    "    Args:\n",
    "        input (List of Tuples of Arrays, Strings): The RAPSD of the\n",
    "        target and predictions and the name to be used in the legend for the predictions\n",
    "        filename (String): Save as this name.pdf\n",
    "    '''\n",
    "\n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, ax = pplt.subplot(xlabel='Wavelength (km)', ylabel='Power (db)',figheight=height, figwidth=\"14cm\")\n",
    "\n",
    "    for pred in input:\n",
    "        ax.plot(pred[0][0], pred[0][1], label = pred[1])\n",
    "    \n",
    "    ax.legend(loc=\"lower left\")\n",
    "    \n",
    "    pplt.show()\n",
    "    #Save it to a subdirectory\n",
    "    file_path = Path.cwd() / (\"plots\")/ filename\n",
    "    fig.savefig(file_path)\n",
    "    pplt.close()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def plot_fss(fss_list, label, filename, title=\"\", create_legend=True, height=\"5cm\"):\n",
    "    '''\n",
    "    Plots the FSS in one bar plot\n",
    "    Input:\n",
    "        fss_list (np-arrays): FSSs, shape(score, window, th)\n",
    "        labels (List of Strings): The name of the FSSs, in order\n",
    "        filename(String): How to save the file\n",
    "    '''\n",
    "    fss_shape = fss_list.shape\n",
    "    data = np.reshape(fss_list, (fss_shape[0]*fss_shape[1], fss_shape[2]))\n",
    "    \n",
    "    data = pd.DataFrame(\n",
    "    data, columns=pd.Index(labels, name='column'),\n",
    "    index=pd.Index([\"1x1, 0.1\", \"1x1, 1\", \"1x1, 2\", \"4x4, 0.1\", \"4x4, 1\", \"4x4, 2\",], name='row idx'))\n",
    "\n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, ax = pplt.subplot(ylabel='Fractions Skill Score', xlabel=\"Window size (pixels) and threshold (mm/hour)\",figheight=height, figwidth=\"14cm\", suptitle=title)\n",
    "    if create_legend:\n",
    "        obj = ax.bar(data, cycle='Blues', edgecolor='black', autoformat=True, labels=label, legend=\"b\")\n",
    "    else:\n",
    "        obj = ax.bar(data, cycle='Blues', edgecolor='black', autoformat=True, labels=label)\n",
    "    pplt.show()\n",
    "    file_path = Path.cwd() / (\"plots\")/ filename\n",
    "    fig.savefig(file_path)\n",
    "    pplt.close()\n",
    "\n",
    "#plotting\n",
    "def plot_specific_prediction(data, model, epoch, titles, k=-1):\n",
    "    if (len(model) == 1):\n",
    "        out = predict(model, epoch[0], data[0])\n",
    "        plot_prediction(data[0][:,4], out, data[1], epoch, \"out\", k)\n",
    "    else:\n",
    "        out = []\n",
    "        for m, e in zip(model, epoch):\n",
    "            out.append(predict(m, e, data[0]))\n",
    "        plot_prediction_multiple_models(data[0][:,4], out, data[1], epoch, \"out\", titles, k)\n",
    "\n",
    "def plot_prediction_multiple_models(pre, preds, y, epoch, prefix, titles, k=-1):\n",
    "    \"\"\"\n",
    "    Plotting the prediction precipitation and saving it as pdf\n",
    "    Args:\n",
    "        pre (tensor): The steps preceding the prediction\n",
    "        pred (List of tensors): The predictions\n",
    "        y (tensor): The ground truth of the prediction\n",
    "        epoch (int): The epoch being plotted\n",
    "        prefix (String): Prefix\n",
    "        k (int): Which item of the batch to plot\n",
    "        titles (List of Strings)\n",
    "    \"\"\"\n",
    "    #We might be using CUDA tensors, so copy the data to the CPU first\n",
    "    y = y.cpu()\n",
    "    pre = pre.cpu()\n",
    "\n",
    "    #if index is invalid or empty, pick a random one\n",
    "    if (k < 0 or k >= y.shape[0]):\n",
    "        k = random.randint(0, y.shape[0]-1)\n",
    "\n",
    "\n",
    "    \n",
    "    #For easier handling, remove the feature dimension, as only precipitation is left\n",
    "    y = torch.squeeze(y[k])\n",
    "    preds_np = []\n",
    "    for pred in preds:\n",
    "        pred = pred.cpu()\n",
    "        preds_np.append(torch.squeeze(pred[k]).numpy())\n",
    "    pre = pre[k] #it is already squeezed\n",
    "\n",
    "    y = torch.cat((pre, y)).numpy() #merge them\n",
    "\n",
    "    n = preds_np[0].shape[0] #how many steps do we predict?\n",
    "    max_prec = 3\n",
    "    \n",
    "    print(\"rows: \" + str(1+len(preds)))\n",
    "    print(\"cols: \" + str(n + 2))\n",
    "\n",
    "    ###############This is where the plotting begins:\n",
    "    with pplt.rc.context(fontsize='10px'):\n",
    "        fig, axes = pplt.subplots(nrows = (1+len(preds)), ncols = n + 2, \n",
    "                                  hspace='1.7em', wspace = '0.4em', figwidth='14cm', titlepad = 7)\n",
    "        #14cm is the text width\n",
    "    fig.format(rowlabels = titles)\n",
    "    \n",
    "    axes.format(\n",
    "    xticks='null', yticks='null', facecolor='gray5')\n",
    "\n",
    "    col = 0\n",
    "    for ax in axes[:n+2]:\n",
    "        #print(\"col:\" + str(col) + \" | row:\" + str(row))\n",
    "        im = ax.pcolormesh(y[col], cmap = 'turbo', vmin = 0, vmax = max_prec)\n",
    "        if(col > 1): #the first two should be empty\n",
    "            ax.format(titleloc = 'center', title = (\"t+\" + str(col-1)))\n",
    "        else:\n",
    "            ax.format(titleloc = 'center', title = (\"t\" + str(col-2)))\n",
    "        col = col+1\n",
    "\n",
    "\n",
    "    row = 1\n",
    "    for pred in preds_np:\n",
    "        col = 0\n",
    "        for ax in axes[row*(n+2):((row+1)*(n+2))]:\n",
    "            if(col > 1): #the first two should be empty\n",
    "                im = ax.pcolormesh(pred[col-2], cmap = 'turbo',vmin = 0, vmax = max_prec)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "            col = col+1\n",
    "        row = row+1\n",
    "\n",
    "    fig.colorbar(im, label='Precipitation in mm/h', loc='r', rows = (1,4), space = 1)#this is hardcoded and prone to fail\n",
    "\n",
    "    #Save it to a subdirectory\n",
    "    file_path = Path.cwd() / (\"plots\")/ (prefix + str(epoch) + \"_prediction_\" + str(k))\n",
    "    \n",
    "    fig.savefig(file_path)\n",
    "    pplt.close()\n",
    "\n",
    "def plot_prediction(pre, pred, y, epoch, prefix, k=-1):\n",
    "    \"\"\"\n",
    "    Plotting the prediction precipitation and saving it as pdf\n",
    "    Args:\n",
    "        pre (tensor): The steps preceding the prediction\n",
    "        pred (tensor): The prediction\n",
    "        y (tensor): The ground truth of the prediction\n",
    "        epoch (int): The epoch being plotted\n",
    "        prefix (String): Prefix\n",
    "        k (int): Which item of the batch to plot\n",
    "    \"\"\"\n",
    "    #We might be using CUDA tensors, so copy the data to the CPU first\n",
    "    pred = pred.cpu()\n",
    "    y = y.cpu()\n",
    "    pre = pre.cpu()\n",
    "\n",
    "    #if index is invalid or empty, pick a random one\n",
    "    if (k < 0 or k >= pred.shape[0]):\n",
    "        k = random.randint(0, pred.shape[0]-1)\n",
    "    \n",
    "    #For easier handling, remove the feature dimension, as only precipitation is left\n",
    "    y = torch.squeeze(y[k])\n",
    "    pred = torch.squeeze(pred[k]).numpy()\n",
    "    pre = pre[k] #it is already squeezed\n",
    "\n",
    "    y = torch.cat((pre, y)).numpy() #merge them\n",
    "\n",
    "    #We use this for scaling the axis\n",
    "    max_prec = max(np.max(y), np.max(pred))\n",
    "\n",
    "    #temp\n",
    "    max_prec = 3\n",
    "    \n",
    "    n = pred.shape[0] #how many steps do we predict?\n",
    "\n",
    "    ###############This is where the plotting begins:\n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, axes = pplt.subplots(nrows = 2, ncols = n + 2, \n",
    "                                  hspace='2.4em', wspace = '0.4em', figwidth='14cm', titlepad = 7)\n",
    "        #14cm is the text width\n",
    "    fig.format(rowlabels = (\"Ground truth\", \"Prediction\"))\n",
    "    \n",
    "    axes.format(\n",
    "    xticks='null', yticks='null', facecolor='gray5')\n",
    "\n",
    "    col = 0\n",
    "    for ax in axes[:n+2]:\n",
    "        im = ax.pcolormesh(y[col], cmap = 'turbo', vmin = 0, vmax = max_prec)\n",
    "        col = col+1\n",
    "\n",
    "    col = 0\n",
    "    for ax in axes[n+2:]:\n",
    "        if(col > 1): #the first two should be empty\n",
    "            im = ax.pcolormesh(pred[col-2], cmap = 'turbo',vmin = 0, vmax = max_prec)\n",
    "            ax.format(titleloc = 'center', title = (\"t+\" + str(col-1)))\n",
    "            col = col+1\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "            ax.format(titleloc = 'center', title = (\"t\" + str(col-2)))\n",
    "            col = col+1\n",
    "\n",
    "    fig.colorbar(im, label='Precipitation in mm/h', loc='r', rows = (1,2), space = 1)\n",
    "\n",
    "    #Save it to a subdirectory\n",
    "    file_path = Path.cwd() / (\"plots\")/ (prefix + str(epoch) + \"_prediction_\" + str(k))\n",
    "    \n",
    "    fig.savefig(file_path)\n",
    "    pplt.close()\n",
    "    \n",
    "def plot_losses():\n",
    "    epochs = 200\n",
    "    with pplt.rc.context(fontsize='11px'):\n",
    "        fig, axes = pplt.subplots(nrows = 3, ncols = 1,figheight='10cm', figwidth=\"14cm\", \n",
    "                                  xlabel='Epochs', ylabel='Average loss', abc=True)\n",
    "\n",
    "    train_l = read_file(\"train_loss_list\", \"H\")\n",
    "    test_l = read_file(\"test_loss_list\", \"H\")\n",
    "    axes[0].plot(train_l, label = \"training\")\n",
    "    axes[0].plot(test_l, label = \"test\")\n",
    "    axes[0].format(title=\"MSE\", titleloc=\"uc\")\n",
    "\n",
    "    train_l = read_file(\"train_loss_list\", \"J\")\n",
    "    test_l = read_file(\"test_loss_list\", \"J\")\n",
    "    axes[1].plot(train_l, label = \"training\")\n",
    "    axes[1].plot(test_l, label = \"test\")\n",
    "    axes[1].format(title=\"Weighted MSE\", titleloc=\"uc\")\n",
    "    axes[1].legend(loc=\"upper right\")\n",
    "\n",
    "    train_l = read_file(\"train_loss_list\", \"K\")\n",
    "    test_l = read_file(\"test_loss_list\", \"K\")\n",
    "    axes[2].plot(train_l, label = \"training\")\n",
    "    axes[2].plot(test_l, label = \"test\")\n",
    "    axes[2].format(title=\"LogCoshLoss\", titleloc=\"uc\")\n",
    "    fig.savefig(\"training\")\n",
    "    pplt.show()\n",
    "    pplt.close()\n",
    "\n",
    "#write list to binary file\n",
    "def write_file(a_list, filename, subdir = \"\"):\n",
    "    #store list in binary file so 'wb' mode\n",
    "    if (subdir == \"\"):\n",
    "        with open(filename, 'wb') as fp:\n",
    "            pickle.dump(a_list, fp)\n",
    "    else:\n",
    "        file_path = Path.cwd() / subdir / filename\n",
    "        file_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        with open(file_path, 'wb') as fp:\n",
    "            pickle.dump(a_list, fp)\n",
    "\n",
    "#read list to memory\n",
    "def read_file(filename, subdir = \"\"):\n",
    "    #store list in binary file so 'wb' mode\n",
    "    if (subdir == \"\"):\n",
    "        with open(filename, 'rb') as fp:\n",
    "            n_list = pickle.load(fp)\n",
    "            return n_list\n",
    "    else:\n",
    "        file_path = Path.cwd() / subdir / filename\n",
    "        with open(file_path, 'rb') as fp:\n",
    "            n_list = pickle.load(fp)\n",
    "            return n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9e000-55d3-422c-8de8-8127bfe66b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses()\n",
    "find_lowest_loss(\"H\")\n",
    "find_lowest_loss(\"J\")\n",
    "find_lowest_loss(\"K\")\n",
    "\n",
    "val = np_to_torch(\"test_array_10_1000\")\n",
    "pgw05 = np_to_torch(\"PGW_10_1000_05\")\n",
    "pgw10 = np_to_torch(\"PGW_10_1000_10\")\n",
    "pgw20 = np_to_torch(\"PGW_10_1000_20\")\n",
    "pgw30 = np_to_torch(\"PGW_10_1000_30\")\n",
    "pgw40 = np_to_torch(\"PGW_10_1000_40\")\n",
    "pgw50 = np_to_torch(\"PGW_10_1000_50\")\n",
    "\n",
    "outH = predict(\"H\", 13, val[0])\n",
    "outJ = predict(\"J\", 36, val[0])\n",
    "outK = predict(\"K\", 20, val[0])\n",
    "\n",
    "#MSE\n",
    "H10 = predict(\"H\", 13, pgw10[0])\n",
    "H30 = predict(\"H\", 13, pgw30[0])\n",
    "H50 = predict(\"H\", 13, pgw50[0])\n",
    "fssH10 = score_fss(pgw10[1], H10)\n",
    "fssH30 = score_fss(pgw30[1], H30)\n",
    "fssH50 = score_fss(pgw50[1], H50)\n",
    "\n",
    "#predictions for weighted MSE\n",
    "J10 = predict(\"J\", 36, pgw10[0])\n",
    "J30 = predict(\"J\", 36, pgw30[0])\n",
    "J50 = predict(\"J\", 36, pgw50[0])\n",
    "fssJ10 = score_fss(pgw10[1], J10)\n",
    "fssJ30 = score_fss(pgw30[1], J30)\n",
    "fssJ50 = score_fss(pgw50[1], J50)\n",
    "\n",
    "#predictions for Logcoshloss\n",
    "K10 = predict(\"K\", 20, pgw10[0])\n",
    "K30 = predict(\"K\", 20, pgw30[0])\n",
    "K50 = predict(\"K\", 20, pgw50[0])\n",
    "fssK10 = score_fss(pgw10[1], K10)\n",
    "fssK30 = score_fss(pgw30[1], K30)\n",
    "fssK50 = score_fss(pgw50[1], K50)\n",
    "\n",
    "plot_rapsd([(score_rapsd(val[1], True), \"Ground truth\"), \n",
    "            (score_rapsd(outH, True), \"MSE\"), (score_rapsd(outJ, True), \"Weighted MSE\"), \n",
    "            (score_rapsd(outK, True), \"LogCoshLoss\"),\n",
    "            (score_rapsd(pgw50[1], True), \"5K-PGW\"), (score_rapsd(J50, True), \"Weighted MSE, 5K\")], \"RAPSD\", \"5cm\")\n",
    "\n",
    "plot_rapsd([(score_rapsd(val[1]), \"Ground truth\"), \n",
    "            (score_rapsd(outH), \"MSE\"), (score_rapsd(outJ), \"Weighted MSE\"), \n",
    "            (score_rapsd(outK), \"LogCoshLoss\"),\n",
    "            (score_rapsd(pgw50[1]), \"5K-PGW\"), (score_rapsd(J50), \"Weighted MSE, 5K\")], \"RAPSD-norm\", \"5cm\")\n",
    "\n",
    "plot_rapsd([(score_rapsd(pgw30[1], True), \"PGW output, 3K\"), \n",
    "            (score_rapsd(H30, True), \"MSE\"), (score_rapsd(J30, True), \"Weighted MSE\"),\n",
    "            (score_rapsd(K30, True), \"LogCoshLoss\")], \"RAPSD-3K\", \"5cm\")\n",
    "\n",
    "plot_rapsd([(score_rapsd(pgw50[1], True), \"PGW output, 5K\"), \n",
    "            (score_rapsd(H50, True), \"MSE\"), (score_rapsd(J50, True), \"Weighted MSE\"),\n",
    "            (score_rapsd(K50, True), \"LogCoshLoss\")], \"RAPSD-5K\", \"5cm\")\n",
    "\n",
    "fssH = score_fss(val[1], outH)\n",
    "fssJ = score_fss(val[1], outJ)\n",
    "fssK = score_fss(val[1], outK)\n",
    "\n",
    "fss_np = np.stack((fssH,fssJ,fssK), axis=2)\n",
    "labels = [\"MSE\", \"WMSE\", \"LogCoshLoss\"]\n",
    "plot_fss(fss_np, labels, \"model_fss\", \"\", True, \"5.6cm\")\n",
    "fss_np = np.stack((fssH,fssH10,fssH30,fssH50), axis=2)\n",
    "labels = [\"Base\", \"1K\", \"3K\", \"5K\"]\n",
    "plot_fss(fss_np, labels, \"warming_fss_H\", \"MSE\", False)\n",
    "fss_np = np.stack((fssJ,fssJ10,fssJ30,fssJ50), axis=2)\n",
    "labels = [\"Base\", \"1K\", \"3K\", \"5K\"]\n",
    "plot_fss(fss_np, labels, \"warming_fss_J\", \"WMSE\", False)\n",
    "fss_np = np.stack((fssK,fssK10,fssK30,fssK50), axis=2)\n",
    "labels = [\"Base\", \"1K\", \"3K\", \"5K\"]\n",
    "plot_fss(fss_np, labels, \"warming_fss_K\", \"LogCoshLoss\", True, \"6.3cm\")\n",
    "\n",
    "#And now for the overfitted model:\n",
    "#predictions for weighted MSE\n",
    "outJO = predict(\"J\", 199, val[0])\n",
    "fssJO = score_fss(val[1], outJO)\n",
    "J10O = predict(\"J\", 199, pgw10[0])\n",
    "J30O = predict(\"J\", 199, pgw30[0])\n",
    "J50O = predict(\"J\", 199, pgw50[0])\n",
    "fssJ10O = score_fss(pgw10[1], J10O)\n",
    "fssJ30O = score_fss(pgw30[1], J30O)\n",
    "fssJ50O = score_fss(pgw50[1], J50O)\n",
    "\n",
    "fss_np = np.stack((fssJO, fssJ10O, fssJ30O, fssJ50O), axis=2)\n",
    "labels = [\"Base\", \"1K\", \"3K\", \"5K\"]\n",
    "plot_fss(fss_np, labels, \"warming_j_fss_overfit\", \"WMSE, overfit\", True, \"6cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd199d-4067-4cb1-bfee-70a10794249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = LogCoshLossOld()\n",
    "train_model(200, loss_fn, optimizer, \"K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
